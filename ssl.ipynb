
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import random
import time # For simple timing

# --- Configuration ---
DATA_PATH = 'arp_train_dataset.csv'
TEST_DATA_PATH = 'arp_test_dataset.csv'

FEATURES = [
    'frame_len', 'arp_opcode',
    'arp_src_hw_mac', 'arp_src_proto_ipv4',
    'arp_dst_hw_mac', 'arp_dst_proto_ipv4'
]
LABEL_COLUMN = 'label'

# Paramètres SSL
MASKING_FRACTION = 0.20
SSL_EPOCHS = 10
SSL_BATCH_SIZE = 64
SSL_LEARNING_RATE = 1e-3

# Paramètres Downstream (Classification)
DS_EPOCHS = 10
DS_BATCH_SIZE = 64
DS_LEARNING_RATE = 1e-3

# Paramètres Modèle
EMBEDDING_DIM = 64
HIDDEN_DIM = 128


# --- Fonction de Prétraitement  ---
def preprocess_data(df, features_list, label_col, num_features, cat_features_onehot, addr_features_labelenc, scaler_obj, onehot_enc_obj, label_enc_objs):
    X = df[features_list].copy()
    y = df[label_col] if label_col in df.columns else None

    # --- Adresser FutureWarning: S'assurer que les features numériques sont float AVANT scaling ---
    try:
        X.loc[:, num_features] = X.loc[:, num_features].astype(np.float64)
    except Exception as e:
        print(f"Avertissement: Impossible de caster les features numériques en float64 avant scaling: {e}")
    # --- Fin  FutureWarning ---

    # Normaliser numériques
    X.loc[:, num_features] = scaler_obj.transform(X[num_features])

    # Encoder catégorielles (OneHot)
    opcode_encoded_np = onehot_enc_obj.transform(X[cat_features_onehot])
    opcode_feature_names = onehot_enc_obj.get_feature_names_out(cat_features_onehot)
    X_opcode = pd.DataFrame(opcode_encoded_np, columns=opcode_feature_names, index=X.index)

    # Encoder adresses (Label)
    for col in addr_features_labelenc:
        # Assurer que la colonne est string avant Label Encoding
        X.loc[:, col] = label_enc_objs[col].transform(X[col].astype(str))
        # Assurer que la sortie est bien entière après Label Encoding
        X.loc[:, col] = X.loc[:, col].astype(np.int64)

    # Concaténer
    X_processed = pd.concat([X[num_features], X_opcode, X[addr_features_labelenc]], axis=1)

    # Construire les noms finaux
    final_feature_names = []
    final_feature_names.extend(num_features)
    final_feature_names.extend(opcode_feature_names)
    final_feature_names.extend(addr_features_labelenc)

    # Assigner et vérifier
    if len(X_processed.columns) == len(final_feature_names):
        X_processed.columns = final_feature_names
    else:
         raise ValueError(f"Incohérence de noms de colonnes lors du prétraitement.")

    # --- Convertir explicitement tout le DataFrame en float32 ---
    try:
        # Vérifier les types AVANT conversion pour aider au débogage si ça échoue
        # print("\nTypes dans X_processed AVANT conversion en float32:")
        # print(X_processed.dtypes)
        X_processed = X_processed.astype(np.float32)
        # print("\nTypes dans X_processed APRES conversion en float32:")
        # print(X_processed.dtypes)
    except Exception as e:
        print("\nERREUR: Impossible de convertir l'ensemble de X_processed en float32.")
        print("Cela signifie probablement qu'une colonne contient encore des valeurs non numériques (ex: strings, objets).")
        print("Vérifiez les étapes de prétraitement (encodage notamment) et les données initiales.")
        print("Types de données actuels dans X_processed:")
        print(X_processed.info()) # Afficher les types pour inspection
        print(f"Erreur spécifique: {e}")
        raise e
    

    # Convertir en tenseurs
    X_tensor = torch.tensor(X_processed.values, dtype=torch.float32)
    y_tensor = torch.tensor(y.values, dtype=torch.long) if y is not None else None

    return X_tensor, y_tensor, X_processed.shape[1]


# --- 1. Chargement et Application du Prétraitement ---
print("--- 1. Chargement et Application du Prétraitement ---")
start_time = time.time()

# Charger les datasets générés
try:
    df_train = pd.read_csv(DATA_PATH)
    df_test = pd.read_csv(TEST_DATA_PATH)
    print(f"Dataset entraînement chargé : {df_train.shape}")
    print(f"Dataset test chargé : {df_test.shape}")
    # Concaténer temporairement pour fitter les encoders/scalers sur toutes les données
    df_full = pd.concat([df_train, df_test], ignore_index=True)
    print(f"Dataset complet (pour fit) : {df_full.shape}")

except FileNotFoundError as e:
    print(f"ERREUR: Fichier dataset non trouvé. Assurez-vous que '{DATA_PATH}' et '{TEST_DATA_PATH}' existent.")
    print("Veuillez exécuter le script de génération de données ('generate_arp_data.py') d'abord.")
    exit()

# --- Fitter les outils de prétraitement sur l'ensemble des données ---
X_full = df_full[FEATURES].copy()
numerical_features = ['frame_len']
address_features = ['arp_src_hw_mac', 'arp_src_proto_ipv4', 'arp_dst_hw_mac', 'arp_dst_proto_ipv4']

# Initialiser
scaler = StandardScaler()
onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
label_encoders = {col: LabelEncoder() for col in address_features}

# Fitter
try:
    X_full.loc[:, numerical_features] = X_full.loc[:, numerical_features].astype(np.float64) # Cast avant fit
    scaler.fit(X_full[numerical_features])
except Exception as e:
     print(f"Erreur lors du fit du scaler: {e}")
     # Gérer l'erreur ou arrêter
     exit()

try:
    onehot_encoder.fit(X_full[['arp_opcode']])
except Exception as e:
     print(f"Erreur lors du fit du OneHotEncoder: {e}")
     exit()

try:
    for col in address_features:
        label_encoders[col].fit(X_full[col].astype(str))
except Exception as e:
     print(f"Erreur lors du fit du LabelEncoder pour {col}: {e}")
     exit()

print("Scalers et Encoders fittés sur l'ensemble des données.")

# Appliquer le prétraitement aux sets Train et Test séparément
try:
    X_train_tensor, y_train_tensor, input_dim = preprocess_data(
        df_train, FEATURES, LABEL_COLUMN, numerical_features, ['arp_opcode'], address_features,
        scaler, onehot_encoder, label_encoders
    )
    X_test_tensor, y_test_tensor, _ = preprocess_data(
        df_test, FEATURES, LABEL_COLUMN, numerical_features, ['arp_opcode'], address_features,
        scaler, onehot_encoder, label_encoders
    )
except Exception as e:
    print(f"Une erreur est survenue lors de l'application du prétraitement: {e}")
    exit()


print(f"\nDimensionnalité après prétraitement: {input_dim}")
print(f"Tenseur Train X: {X_train_tensor.shape}, Tenseur Train Y: {y_train_tensor.shape if y_train_tensor is not None else 'N/A'}")
print(f"Tenseur Test X: {X_test_tensor.shape}, Tenseur Test Y: {y_test_tensor.shape if y_test_tensor is not None else 'N/A'}")
preprocessing_time = time.time() - start_time
print(f"Temps de chargement et prétraitement: {preprocessing_time:.2f} secondes")


# --- 2. Définition du Modèle ---

print("\n--- 2. Définition du Modèle ---")
class ARP_SSL_Model(nn.Module):
    def __init__(self, input_dim, embedding_dim, hidden_dim):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, embedding_dim)
        )
        self.ssl_head = nn.Linear(embedding_dim, input_dim)
        self.downstream_head = nn.Linear(embedding_dim, 2)

    def forward(self, x, task='encode'):
        embedding = self.encoder(x)
        if task == 'ssl':
            return self.ssl_head(embedding)
        elif task == 'downstream':
            return self.downstream_head(embedding)
        elif task == 'encode':
            return embedding
        else:
            raise ValueError("Task must be 'ssl', 'downstream', or 'encode'")

model = ARP_SSL_Model(input_dim, EMBEDDING_DIM, HIDDEN_DIM)
print(model)
total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Nombre total de paramètres entraînables: {total_params:,}")


# --- 3. Tâche SSL : Prédiction de Caractéristiques Masquées ---
# 
print("\n--- 3. Pré-entraînement SSL (Masked Feature Prediction) ---")
start_time_ssl = time.time()
class MaskedDataset(Dataset):
    def __init__(self, data_tensor, mask_fraction=0.2, mask_value=0.0):
        self.data = data_tensor
        self.mask_fraction = mask_fraction
        self.num_features = data_tensor.shape[1]
        self.mask_value = mask_value

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        original_sample = self.data[idx].clone()
        masked_sample = original_sample.clone()
        num_mask = int(self.num_features * self.mask_fraction)
        if num_mask == 0:
             if self.mask_fraction > 0: num_mask = 1
             else: return masked_sample, original_sample, torch.zeros(self.num_features, dtype=torch.float32)
        mask_indices = random.sample(range(self.num_features), num_mask)
        masked_sample[mask_indices] = self.mask_value
        mask = torch.zeros(self.num_features, dtype=torch.float32)
        mask[mask_indices] = 1.0
        return masked_sample, original_sample, mask

ssl_dataset = MaskedDataset(X_train_tensor, MASKING_FRACTION)
ssl_dataloader = DataLoader(ssl_dataset, batch_size=SSL_BATCH_SIZE, shuffle=True)
ssl_criterion = nn.MSELoss(reduction='none')
ssl_optimizer = optim.Adam(model.parameters(), lr=SSL_LEARNING_RATE)
ssl_losses = []
model.train()
print(f"Début de l'entraînement SSL pour {SSL_EPOCHS} époques...")
for epoch in range(SSL_EPOCHS):
    epoch_loss = 0.0
    processed_batches = 0
    for masked_batch, original_batch, mask_batch in ssl_dataloader:
        ssl_optimizer.zero_grad()
        predictions = model(masked_batch, task='ssl')
        loss_per_feature = ssl_criterion(predictions, original_batch)
        mask_sum = mask_batch.sum()
        masked_loss = (loss_per_feature * mask_batch).sum() / mask_sum.clamp(min=1e-6)
        if not torch.isnan(masked_loss) and not torch.isinf(masked_loss):
             masked_loss.backward()
             ssl_optimizer.step()
             epoch_loss += masked_loss.item()
        else:
             print(f"Attention: Perte NaN ou Inf détectée à l'époque {epoch+1}, batch sauté.")
        processed_batches += 1
        if processed_batches % 20 == 0:
             print(f"\rSSL Epoch [{epoch+1}/{SSL_EPOCHS}], Batch [{processed_batches}/{len(ssl_dataloader)}], Loss: {masked_loss.item():.4f}", end='')
    avg_epoch_loss = epoch_loss / len(ssl_dataloader) if len(ssl_dataloader) > 0 else 0
    ssl_losses.append(avg_epoch_loss)
    print(f"\rSSL Epoch [{epoch+1}/{SSL_EPOCHS}], Average Loss: {avg_epoch_loss:.4f}       ")
ssl_training_time = time.time() - start_time_ssl
print(f"Pré-entraînement SSL terminé. Temps: {ssl_training_time:.2f} secondes")
plt.figure(figsize=(10, 5))
plt.plot(range(1, SSL_EPOCHS + 1), ssl_losses, marker='o', linestyle='-')
plt.title("Courbe de Perte du Pré-entraînement SSL")
plt.xlabel("Époque")
plt.ylabel("Perte MSE Moyenne (sur features masquées)")
plt.grid(True)
plt.xticks(range(1, SSL_EPOCHS + 1))
plt.show()


# --- . Tâche Downstream : Détection d'ARP Spoofing (Classification) ---

print("\n--- 4. Fine-tuning / Évaluation Downstream (Classification) ---")
start_time_ds = time.time()
if y_train_tensor is None or y_test_tensor is None:
    print("Labels manquants dans les données d'entraînement ou de test. Impossible de faire l'évaluation downstream.")
else:
    class SimpleDataset(Dataset):
        def __init__(self, features, labels):
            self.features = features
            self.labels = labels
        def __len__(self):
            return len(self.features)
        def __getitem__(self, idx):
            return self.features[idx], self.labels[idx]
    ds_train_dataset = SimpleDataset(X_train_tensor, y_train_tensor)
    ds_test_dataset = SimpleDataset(X_test_tensor, y_test_tensor)
    ds_train_loader = DataLoader(ds_train_dataset, batch_size=DS_BATCH_SIZE, shuffle=True)
    ds_test_loader = DataLoader(ds_test_dataset, batch_size=DS_BATCH_SIZE, shuffle=False)
    print("Mode: Entraînement de la tête de classification seulement (encodeur gelé).")
    for param in model.encoder.parameters():
        param.requires_grad = False
    for param in model.downstream_head.parameters():
        param.requires_grad = True
    ds_criterion = nn.CrossEntropyLoss()
    ds_optimizer = optim.Adam(model.downstream_head.parameters(), lr=DS_LEARNING_RATE)
    model.train()
    print(f"Début de l'entraînement Downstream pour {DS_EPOCHS} époques...")
    ds_losses = []
    for epoch in range(DS_EPOCHS):
        epoch_loss = 0.0
        processed_batches = 0
        for features, labels in ds_train_loader:
            ds_optimizer.zero_grad()
            outputs = model(features, task='downstream')
            loss = ds_criterion(outputs, labels)
            if not torch.isnan(loss) and not torch.isinf(loss):
                 loss.backward()
                 ds_optimizer.step()
                 epoch_loss += loss.item()
            else:
                 print(f"Attention: Perte NaN ou Inf détectée à l'époque {epoch+1}, batch sauté.")
            processed_batches += 1
            if processed_batches % 20 == 0:
                 print(f"\rDS Epoch [{epoch+1}/{DS_EPOCHS}], Batch [{processed_batches}/{len(ds_train_loader)}], Loss: {loss.item():.4f}", end='')
        avg_epoch_loss = epoch_loss / len(ds_train_loader) if len(ds_train_loader) > 0 else 0
        ds_losses.append(avg_epoch_loss)
        print(f"\rDS Epoch [{epoch+1}/{DS_EPOCHS}], Average Loss: {avg_epoch_loss:.4f}         ")
    ds_training_time = time.time() - start_time_ds
    print(f"Entraînement Downstream terminé. Temps: {ds_training_time:.2f} secondes")
    plt.figure(figsize=(10, 5))
    plt.plot(range(1, DS_EPOCHS + 1), ds_losses, marker='x', linestyle='--')
    plt.title("Courbe de Perte de l'Entraînement Downstream (Tête de Classification)")
    plt.xlabel("Époque")
    plt.ylabel("Perte CrossEntropy Moyenne")
    plt.grid(True)
    plt.xticks(range(1, DS_EPOCHS + 1))
    plt.show()
    print("\n--- Évaluation sur le Set de Test ---")
    model.eval()
    all_preds = []
    all_labels = []
    all_probs = []
    with torch.no_grad():
        for features, labels in ds_test_loader:
            outputs = model(features, task='downstream')
            probs = torch.softmax(outputs, dim=1)
            _, predicted = torch.max(outputs.data, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, pos_label=1, zero_division=0)
    recall = recall_score(all_labels, all_preds, pos_label=1, zero_division=0)
    f1 = f1_score(all_labels, all_preds, pos_label=1, zero_division=0)
    cm = confusion_matrix(all_labels, all_preds)
    print("\n--- Résultats de l'Évaluation Downstream (Test Set) ---")
    print(f"Accuracy:  {accuracy:.4f}")
    print(f"Precision (pour classe 1 - Spoofing): {precision:.4f}")
    print(f"Recall (pour classe 1 - Spoofing):    {recall:.4f}")
    print(f"F1-Score (pour classe 1 - Spoofing):  {f1:.4f}")
    print("\nMatrice de Confusion:")
    print("          Predicted 0 (Normal)  Predicted 1 (Spoofing)")
    print(f"Actual 0 (Normal)    {cm[0, 0]:<18}  {cm[0, 1]:<18}")
    print(f"Actual 1 (Spoofing)  {cm[1, 0]:<18}  {cm[1, 1]:<18}")
    plt.figure(figsize=(7, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Normal (0)', 'Spoofing (1)'],
                yticklabels=['Normal (0)', 'Spoofing (1)'])
    plt.xlabel('Prédiction')
    plt.ylabel('Valeur Réelle')
    plt.title('Matrice de Confusion (Test Set)')
    plt.show()

print("\n--- Fin du Script ---")
total_runtime = time.time() - start_time # Calculer le temps total
print(f"Temps d'exécution total: {total_runtime:.2f} secondes")
